#!/usr/bin/env python

import sys, gc, os, csv
from yaml import CBaseLoader
from argparse import ArgumentError, ArgumentParser

from bl.vl.kb import KnowledgeBase as KB
from bl.vl.kb import KBError
from bl.vl.kb.serialize.deserialize import ObjectsLimbo
from bl.vl.utils import LOG_LEVELS, get_logger


def make_parser():
    parser = ArgumentParser('Restore data stored as YAML to an OMERO server')
    parser.add_argument('--logfile', type=str, help='log file (default=stderr)')
    parser.add_argument('--loglevel', type=str, choices=LOG_LEVELS,
                        help='logging level', default='INFO')
    parser.add_argument('--host', '-H', type=str, help='OMERO host',
                        required=True)
    parser.add_argument('--user', '-U', type=str, help='OMERO user',
                        required=True)
    parser.add_argument('--passwd', '-P', type=str, help='OMERO password',
                        required=True)
    parser.add_argument('--yaml-file', '-Y', type=str, help='YAML input file')
    parser.add_argument('--chunks-directory', '-D', type=str,
                        help='directory containing YAML chunks produced using the split_yaml_dump tool')
    parser.add_argument('--public-objects-file', '-p', type=str,
                        help='a TSV file containing the list of the objects that will be moved to the public pool at the end of the restore process')
    return parser


def objects_from_yaml(kb, yaml_file, logger):
    logger.info('Loading objects from %s', yaml_file)
    limbo = ObjectsLimbo(kb, logger)
    with open(yaml_file) as f:
        loader = CBaseLoader(f)
        yaml_data = loader.get_data()
        loader.dispose()
        for ref, conf in yaml_data.iteritems():
            limbo.add_object(ref, conf)
        del yaml_data
        gc.collect()
    return limbo


def records_by_chunk(records, batch_size=500):
    offset = 0
    while len(records[offset:]) > 0:
        yield records[offset:offset+batch_size]
        offset += batch_size


def save_objects(kb, objects, logger, rollback_enabled=False):
    kb.connect()
    logger.info('Saving %d records', len(objects))
    saved_records = []
    for chunk, records in enumerate(records_by_chunk(objects)):
        try:
            logger.debug('Saving chunk %d', chunk)
            saved_records.extend(kb.save_array(records))
            # unload saved objects, this will prevent SecurityViolation errors when dealing
            # with object shared in the public space by other users
            for r in records:
                r.unload()
        except Exception, e:
            logger.exception('An error occurred while saving chunk %d, do a rollback', chunk)
            if rollback_enabled:
                # delete saved objects if an error occurs and exit
                rollback(kb, saved_records)
            raise e
    kb.disconnect()


def rollback(kb, objects):
    for o in objects:
        kb.delete(o)

# Some objects like Studies and Devices can be shared in the biobank, handle them properly
def save_shared_objects(kb, objects, logger):
    kb.connect()
    for o in objects:
        try:
            o.save()
        except KBError, kerr:
            logger.debug('Object %s::%s already exists, ignoring',
                         o.__class__.__name__, o.id)
    kb.disconnect()


def do_preemptive_cleanup(kb, ids, obj_type, logger):
    logger.info('Start preemptive cleanup')
    objs_map = {}
    for id in ids:
        try:
            objs_map[id] = kb.get_by_vid(getattr(kb, obj_type), id)
        except ValueError:
            logger.debug('%s:%s not found', obj_type, id)
    logger.info('Found %d objects, deleting them', len(objs_map))
    for o in objs_map.values():
        kb.delete(o)


def to_biobank(kb, objects_limbo, logger, rollback_enabled=False,
               preemptive_cleanup=False):
    def get_base_klass(objtype):
        import omero
        import omero.model
        klass = getattr(omero.model, objtype)
        for i, k in enumerate(getattr(omero.model, objtype).__mro__):
            if k is omero.model.IObject:
                try:
                    klass = getattr(omero.model, objtype).__mro__[i-1]
                except IndexError:
                    pass
        return klass.__name__
    shared_objects_type = ['Study', 'Device']
    for objtype, objects in objects_limbo.groupbytype():
        logger.info('Saving type %s', objtype)
        bb_objs = [o[1] for o in objects]
        if preemptive_cleanup:
            ids = [o.id for o in bb_objs]
            do_preemptive_cleanup(kb, ids, objtype, logger)
        if get_base_klass(objtype) in shared_objects_type:
            save_shared_objects(kb, bb_objs, logger)
        else:
            save_objects(kb, bb_objs, logger, rollback_enabled)
    logger.info('Objects restoring completed')


def dump_file(kb, file_path, logger, rollback_enabled=False,
              preemptive_cleanup=False):
    try:
        objects_limbo = objects_from_yaml(kb, file_path, logger)
        to_biobank(kb, objects_limbo, logger, rollback_enabled,
                   preemptive_cleanup)
        return True
    except Exception, e:
        logger.exception(e.message)
        return False


def dump_chunks(kb, chunks_folder, logger):
    files = os.listdir(chunks_folder)
    files.sort()
    for f in files:
        dumped = dump_file(kb, os.path.join(chunks_folder, f), logger,
                           rollback_enabled=True, preemptive_cleanup=True)
        if dumped:
            os.remove(os.path.join(chunks_folder, f))


def move_to_common_space(kb, public_objects_file, logger):
    with open(public_objects_file) as f:
        reader = csv.DictReader(f, delimiter='\t')
        records = [r for r in reader]
    logger.debug('Fount %d public objects', len(records))
    public_objects = []
    for rec in records:
        try:
            obj = kb.get_by_vid(getattr(kb, rec['class']), rec['id'])
            # reload if object was stored in cache and unloaded in a previous cycle
            obj.reload()
            # only append objects that are not yet in the common space
            if kb.get_object_group(obj) != 'user':
                public_objects.append(obj)
        except ValueError:
            pass
    if len(public_objects) > 0:
        logger.info('Moving %d objects to common space', len(public_objects))
        kb.admin.move_to_common_space(public_objects)
    else:
        logger.info('There are no objects that need to be moved to common space')


def main(argv):
    parser = make_parser()
    args = parser.parse_args(argv)

    logger = get_logger('restore_from_yaml', level=args.loglevel, filename=args.logfile)
    kb = KB(driver='omero')(args.host, args.user, args.passwd)
    kb.disconnect()

    if not args.yaml_file and not args.chunks_directory:
        raise Exception('Provide a YAML file or a directory containing chunks')
    if args.yaml_file and args.chunks_directory:
        raise Exception('Provide only one between YAML file and chunks directory')

    # if public objects files has been passed as input parameter, check if the current user
    # is owner of its default group
    kb.connect()
    g, _ = kb.get_current_group()
    if not kb.is_group_leader(g):
        raise KBError('User is no leader of group %s and is unable to move objects to common space. Stopping job.' % g)

    if args.yaml_file:
        dump_file(kb, args.yaml_file, logger)
    if args.chunks_directory:
        dump_chunks(kb, args.chunks_directory, logger)

    if args.public_objects_file:
        move_to_common_space(kb, args.public_objects_file, logger)

if __name__ == '__main__':
    main(sys.argv[1:])
