#!/usr/bin/env python

# BEGIN_COPYRIGHT
# END_COPYRIGHT

import argparse
import sqlite3
import csv, os, sys, re
from bs4 import BeautifulSoup
import logging, time

"""

Illumina Sentrix Meta data utility
==================================

The purpose of this utility is to extract information from Illumina
beadchip meta files. The results are accumulated in a sqlite3 file.

"""

DEFAULT_DB_NAME='illumina_chips.db'

LOG_FORMAT = '%(asctime)s|%(levelname)-8s|%(message)s'
LOG_DATEFMT = '%Y-%m-%d %H:%M:%S'
LOG_LEVELS = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']

def cmd_parser():
    parser = argparse.ArgumentParser(description="""
    gather meta information from illumina beadchip files""")
    parser.add_argument('--logfile', type=str, help='log file (default=stderr)')
    parser.add_argument('--loglevel', type=str, choices=LOG_LEVELS,
                        help='logging level (default=INFO)', default='INFO')
    parser.add_argument("--db-file", type=str, default=DEFAULT_DB_NAME,
                        help="name of the db file that should be used")

    parser.add_argument("--reset", action='store_true',
                        help="reset db")
    parser.add_argument("--sdfs", type=argparse.FileType('r'),
                        help="file with sdf file paths (one per line)")
    parser.add_argument("--sdf", type=str,
                        help="a sdf file")
    parser.add_argument("--csvs", type=argparse.FileType('r'),
                        help="file with summary file paths (one per line)")
    parser.add_argument("--csv", type=str,
                        help="a summary file")
    parser.add_argument("--check-consistency", action='store_true',
                        help="check sentrix slots mapping")
    return parser

#----------------------------------------------------------------------------


summary_fields = "Chip_Type Date Sample_ID Sample_Plate Sample_Well Sample_Name Project AMP_Plate SentrixBarcode_A SentrixPosition_A Scanner Date_Scan".split()

sentrix_fields = "SentrixBarcode_A Type Class Assay_Type Rows Cols".split()

def normalize(label):
    m = re.match('^([A-Z])(\d+)$', label.upper())
    if not m:
      raise ValueError('label [%s] not in the form A1' % label)
    return '%s%d' % (m.groups()[0], int(m.groups()[1]))
    

class ingestor(object):
    SUMMARY_FIELDS = zip(summary_fields, ['text'] * len(summary_fields))
    SENTRIX_FIELDS = zip(sentrix_fields,
                         ['text'] * (len(sentrix_fields) - 2) + ['integer'] * 2)
    SUMMARY_TABLE  = 'summaries'
    SENTRIX_TABLE  = 'sentrix'

    def __init__(self, dbname):
        logger.info('setting dbname to {}'.format(dbname))
        self.dbname = dbname
        self.initialize_db(rebuild=False)


    def create_table(self, cursor, tname, field_descs, unique_cols=None):
        fields = ','.join(map(' '.join, field_descs))
        if unique_cols:
            unique_bit = ', UNIQUE ({})'.format(','.join(unique_cols))
        else:
            unique_bit = ''
        sql_cmd = 'CREATE TABLE {} ({} {})'.format(tname, fields, unique_bit)
        logger.debug('{}'.format(sql_cmd))
        cursor.execute(sql_cmd)

    def initialize_db(self, rebuild=True):
        logger.info('initializing db file {}'.format(self.dbname))
        if os.path.exists(self.dbname):
            if rebuild:
                logger.info('removing existing db file {}'.format(self.dbname))
                os.unlink(self.dbname)
            else:
                return
        conn = sqlite3.connect(self.dbname)
        c = conn.cursor()

        u_cols = "SentrixBarcode_A SentrixPosition_A".split()
        self.create_table(c, self.SUMMARY_TABLE, self.SUMMARY_FIELDS, u_cols)

        u_cols = ["SentrixBarcode_A"]
        self.create_table(c, self.SENTRIX_TABLE, self.SENTRIX_FIELDS, u_cols)
        conn.commit()
        conn.close()
        logger.info('done reinitializing db file')

    def build_summary_stream(self, fname):
        def get_field(f, field_name):
            l = f.readline()
            parts = re.split('[,;]', l)
            if not parts[0].startswith(field_name):
                logger.error('missing field: {} in {}'.format(field_name, l))
                sys.exit(1)
            return '' if len(parts) == 1 else parts[1].strip()
        def get_header(f):
            for x in ["[Header]", "Investigator Name",
                      "Project Name", "Experiment Name"]:
                get_field(f, x)
            date = get_field(f, 'Date')
            get_field(f, '[Manifests]')
            # FIXME we have seen only single manifests files thus far...
            chip_type = get_field(f, 'A')
            # convert to string compatible with the kb enum.
            chip_type = chip_type.replace('.bpm', '').replace('-', '_')
            get_field(f, '[Data]')
            return date, chip_type

        s = csv.Sniffer()
        with open(fname) as f:
            date, chip_type = get_header(f)
            pos = f.tell()
            d = s.sniff(f.readline())
            f.seek(pos)
            cf = csv.DictReader(f, dialect=d)
            for r in cf:
                r['Chip_Type'] = chip_type
                r['Date'] = date
                if r['Sample_Plate'] == '':
                    r['Sample_Plate']  = '_'.join(r['Project'].split('_')[:-1])
                r['Sample_Well'] = normalize(r['Sample_Well'])
                yield map(r.__getitem__,
                          map(lambda x: x[0], self.SUMMARY_FIELDS))

    def ingest_summary(self, fname):
        logger.info('ingesting data from {}'.format(fname))
        stream = self.build_summary_stream(fname)
        conn = sqlite3.connect(self.dbname)
        c = conn.cursor()
        for r in stream:
            if r[8].strip () == '':  #'SentrixBarcode_A'
                logger.error('file: {}, record: {} -- BAD SentrixBarcode_A'.format(fname, r))
                continue
            sql_stat = "INSERT INTO {} VALUES {}".format(
                self.SUMMARY_TABLE,
                str(tuple(map(lambda x: "%s" % x, r))))
            logger.debug(sql_stat)
            c.execute(sql_stat)
        conn.commit()
        conn.close()
        logger.info('done ingesting data from {}'.format(fname))

    def ingest_sdf(self, fname):
        logger.info('ingesting data from {}'.format(fname))
        sdf_barcode, ext = os.path.basename(fname).split('.')
        s = BeautifulSoup(open(fname))
        sdf_name = str(s.find_all('name')[0].text)
        sdf_class= str(s.find_all('class')[0].text)
        sdf_assay_type = str(s.find_all('assaytype')[0].text)
        # quick and dirty
        rows, cols = s.samplelabels.find_all('string')[-1].text.split('C')
        rows, cols = int(rows[1:]), int(cols)
        sql_stat = "INSERT INTO {} VALUES {}".format(
            self.SENTRIX_TABLE,
            tuple([sdf_barcode, sdf_name, sdf_class, sdf_assay_type,
                   rows,cols]))
        logger.debug(sql_stat)
        conn = sqlite3.connect(self.dbname)
        c = conn.cursor()
        try:
            c.execute(sql_stat)
            conn.commit()
        except sqlite3.IntegrityError, e:
            logger.error('integrity fail on {}'.format(fname))
        conn.close()
        logger.info('done ingesting data from {}'.format(fname))

    def get_summary_by_barcode(self):
        sql_stat = "select Sample_ID, Sample_plate, Sample_Well, SentrixBarcode_A, SentrixPosition_A from {};".format(
            self.SUMMARY_TABLE)
        logger.debug(sql_stat)
        conn = sqlite3.connect(self.dbname)
        c = conn.cursor()
        try:
            c.execute(sql_stat)
            res = c.fetchall()
        except sqlite3.OperationalError, e:
            logger.error('error: {}'.format(e))
        conn.close()
        by_barcode = {}
        for r in res:
            by_barcode.setdefault(r[3], []).append(r)
        return by_barcode

    def get_sentrix_by_barcode(self):
        sql_stat = "select SentrixBarcode_A, Rows, Cols from {};".format(
            self.SENTRIX_TABLE)
        logger.debug(sql_stat)
        conn = sqlite3.connect(self.dbname)
        c = conn.cursor()
        try:
            c.execute(sql_stat)
            res = c.fetchall()
        except sqlite3.OperationalError, e:
            logger.error('error: {}'.format(e))
        conn.close()
        by_barcode = {}
        for r in res:
            by_barcode.setdefault(r[0], []).append(r)
        return by_barcode

    def check_consistency(self):
        logger.info('checking data consistency')
        summary_by_barcode = self.get_summary_by_barcode()
        sentrix_by_barcode = self.get_sentrix_by_barcode()

        m = max(map(lambda x: len(x), sentrix_by_barcode.values()))
        if m > 1:
            for k in sentrix_by_barcode:
                if len(sentrix_by_barcode[k]) > 1:
                    print k, len(sentrix_by_barcode[k])
        for k in summary_by_barcode:
            if not sentrix_by_barcode.has_key(k):
                print 'undefined key {}'.format(k)

        logger.info('done checking.')


if __name__ == '__main__':
    parser = cmd_parser()
    args = parser.parse_args()
    log_level = getattr(logging, args.loglevel)
    kwargs = {'format': LOG_FORMAT,
              'datefmt': LOG_DATEFMT,
              'level': log_level}
    if args.logfile:
        kwargs['filename'] = args.logfile
    logging.basicConfig(**kwargs)
    logger = logging.getLogger()

    ing = ingestor(args.db_file)
    if args.reset:
        ing.initialize_db()
    if args.sdf:
        ing.ingest_sdf(args.sdf)
    if args.sdfs:
        for l in args.sdfs:
            ing.ingest_sdf(l.strip())
    if args.csv:
        ing.ingest_summary(args.csv)
    if args.csvs:
        for l in args.csvs:
            ing.ingest_summary(l.strip())

    if args.check_consistency:
        ing.check_consistency()


